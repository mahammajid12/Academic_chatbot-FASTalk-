# -*- coding: utf-8 -*-
"""DataCleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_E_tWlI_1b0JWKiNuiG6omHSzRpmnpT

**The code for making the dataset**


> Indented block
"""

from google.colab import drive
drive.mount('/content/drive')

def file_read(fname):
        content_array = []
        with open(fname) as f:
                #Content_list is the list that contains the read lines.     
                for line in f:
                    content_array.append(line)
                #print(content_array)
        return content_array

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('stopwords')
#import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
stopwords = nltk.corpus.stopwords.words('english')

sentences = nltk.sent_tokenize(carr[680]) 
print(sentences)
print(".......")
#sentences=nltk.sent_tokenize("Mr.HassanMustafa is teaching ObjectOrientedProgrammingCS to sectionA Batch2018")
keywords1 = []
for sentence in sentences:
    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):
        print(word)
        print(pos)
        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos=='VB' or pos == 'VBG'):
            if (word.find("CS")!=-1):
              word=word.replace("CS","")
            keywords1.append(word)
    
import re
print("EXTRACTED KEYWORDS")
print(keywords1)
def listToString(s):  
    str1 = " "
    return (str1.join(s)) 

print("...........")
temp=carr[680]
for i in range(len(keywords1)):
  st1=keywords1[i]
  updatedst=re.findall('[A-Z][^A-Z]*', st1)
  print(updatedst)
  stnew=listToString(updatedst)
  #print(st1)
  print(stnew)
  if(st1[0].isupper()):
    temp=temp.replace(st1,stnew)

print(temp)

finalKeyW=[]      ###############################################
for i in range(len(carr)):
  sentences = nltk.sent_tokenize(carr[i]) 
  #finalSentences.append(carr[i])
  keywords1 = ""
  for sentence in sentences:
      for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):
          if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos=='CD' or pos=='VB'):
              #keywords1.append(word)
              keywords1=keywords1+word
              keywords1=keywords1+' '
  finalKeyW.append(keywords1)
print(finalKeyW)

for i in range(len(finalKeyW)):
  finalKeyW[i]=finalKeyW[i].replace(" @ ","@")

print(finalKeyW)

import numpy as np
def file_read(fname):
        content_array = []
        with open(fname) as f:    
                for line in f:
                    content_array.append(line)
                    content_array[-1] = content_array[-1].strip()
                #print(content_array)
        return content_array

sentences=file_read('drive/My Drive/FYP/TrainingData_Text.txt')
for i in range(len(sentences)):
  #sentences[i]=sentences[i].replace("CS","")
  sentences[i]=sentences[i].split()

print(sentences)

from gensim.models import Word2Vec
# define training data

'''sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
			['this', 'is', 'the', 'second', 'sentence'],
			['yet', 'another', 'sentence'],
			['one', 'more', 'sentence'],
			['and', 'the', 'final', 'sentence']]'''
# train model
model = Word2Vec(sentences, min_count=1)
# summarize the loaded model
print(model)
print()
print()
# summarize vocabulary
words = list(model.wv.vocab)
print(words)
print()
# access vector for one word
print(model['sectionA'])



# save model
model.save('model.bin')
# load model
new_model = Word2Vec.load('model.bin')
print(new_model)

newstr=[]
for i in range(len(words)):
  xx=model[words[i]]
  xx=xx[:30]
  str1 = ' '.join(str(x) for x in xx)
  newstr.append(str(words[i])+' '+str1)

print(newstr)

with open('drive/My Drive/FYP/vec5ed.txt', 'w') as filehandle:
    for listitem in newstr:
        filehandle.write('%s\n' % listitem)

xx=model['sectionB']
xx=xx[:20]
print(xx)