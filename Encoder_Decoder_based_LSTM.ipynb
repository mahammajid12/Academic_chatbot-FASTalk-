{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder Decoder based LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXjy05G1U8tO",
        "colab_type": "code",
        "outputId": "e3cefbff-1d5f-4f3d-ce1d-0f75cc0b0e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "import os\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGKd-yhYjW1a",
        "colab_type": "code",
        "outputId": "259a9719-bdd3-49ff-f5b7-26dc3ce24934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSlZ8sBRtCS_",
        "colab_type": "code",
        "outputId": "4d76daa0-331b-4741-85dd-c3b33dbf385a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/data\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificial_intelligence.yml  ExpectedResults.txt    Sport_games.yml\n",
            "computers.yml\t\t     IT.yml\t\t    TextQuestions.txt\n",
            "CourseAllocationCS.yml\t     space_and_science.yml  website.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NTQzTYoexKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphas = 'abcdefghijklmnopqrstuvwxyz1234567890'\n",
        "\n",
        "MAX_INPUT_SEQ_LENGTH = 40\n",
        "MAX_TARGET_SEQ_LENGTH = 40\n",
        "DATA_DIR_PATH = '/content/gdrive/My Drive/data'\n",
        "MAX_VOCAB_SIZE = 30000\n",
        "\n",
        "marker_start = '<begin>'\n",
        "marker_end = '<end>'\n",
        "marker_unknown = '<unk>'\n",
        "marker_pad = '<pad>'\n",
        "\n",
        "# standard step - reset computation graphs\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# 2 more for start and stop markers\n",
        "input_seq_len = 15\n",
        "output_seq_len = input_seq_len+2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC9v66xuexKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defines permissible characters for the chatbot\n",
        "def permissible_chars(word):\n",
        "\n",
        "    for char in word:\n",
        "        if char in alphas:\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZFKl5fNexKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Compute softmax values for each sets of scores in x.\n",
        "def softmax(x): \n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhDDdW2VexKs",
        "colab_type": "code",
        "outputId": "a150869b-6b7d-4510-d57b-07f3a23680a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# To parse the input yml files and create word2index and index2word mappings\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "target_counter = Counter()\n",
        "input_counter = Counter()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# Parser base code of GuntherCox dataset obtained from link below and modified as per requirement.\n",
        "# https://github.com/kushagra2101/ChatCrazie/blob/master/train_seq2seq.py\n",
        "\n",
        "for file in os.listdir('/content/drive/My Drive/data'):\n",
        "    filepath = os.path.join('/content/drive/My Drive/data', file)\n",
        "    if os.path.isfile(filepath):\n",
        "        print('processing file: ', file)\n",
        "        lines = open(filepath, 'rt', encoding='utf8').read().split('\\n')\n",
        "        prev_words = []\n",
        "        for line in lines:\n",
        "\n",
        "            if line.startswith('- - '):\n",
        "                prev_words = []\n",
        "\n",
        "            if line.startswith('- - ') or line.startswith('  - '):\n",
        "                line = line.replace('- - ', '')\n",
        "                line = line.replace('  - ', '')\n",
        "                next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
        "                next_words = [w for w in next_words if permissible_chars(w)]\n",
        "                if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
        "                    next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
        "\n",
        "                if len(prev_words) > 0:\n",
        "                    input_texts.append(prev_words)\n",
        "                    for w in prev_words:\n",
        "                        input_counter[w] += 1\n",
        "\n",
        "                    target_words = next_words[:]\n",
        "                    for w in target_words:\n",
        "                        target_counter[w] += 1\n",
        "                    target_texts.append(target_words)\n",
        "\n",
        "                prev_words = next_words\n",
        "\n",
        "\n",
        "for idx, (input_words, target_words) in enumerate(zip(input_texts, target_texts)):\n",
        "    if idx < 20:\n",
        "        print([input_words, target_words])\n",
        "\n",
        "input_w2i, input_i2w, target_w2i, target_i2w = {},{},{},{}\n",
        "\n",
        "### Creating Word2index and Index2word, forward and reverse mapping ###\n",
        "## we will create dictionaries to provide a unique integer for each word.\n",
        "input_w2i[marker_unknown] = 0\n",
        "input_w2i[marker_pad] = 1\n",
        "# filter out the rare words\n",
        "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
        "    input_w2i[word[0]] = idx+2\n",
        "\n",
        "# inverse dictionary for vocab_to_int.\n",
        "input_i2w = dict([(idx, word) for word, idx in input_w2i.items()])\n",
        "\n",
        "## we will create dictionaries to provide a unique integer for each word.\n",
        "target_w2i[marker_unknown] = 0\n",
        "target_w2i[marker_pad] = 1\n",
        "target_w2i[marker_start] = 2\n",
        "target_w2i[marker_end] = 3\n",
        "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
        "    target_w2i[word[0]] = idx+4\n",
        "\n",
        "# inverse dictionary for vocab_to_int.\n",
        "target_i2w = dict([(idx, word) for word, idx in target_w2i.items()])\n",
        "\n",
        "\n",
        "###########################################\n",
        "\n",
        "# inputVocabLen = len(input_word2idx)\n",
        "# targetVocabLen = len(target_word2idx)\n",
        "\n",
        "###########################################\n",
        "\n",
        "# if the word is not found then default with 0. \n",
        "# 0 in index means the word is unknown (<unk>)\n",
        "x = [[input_w2i.get(word, 0) for word in sentence] for sentence in input_texts]\n",
        "y = [[target_w2i.get(word, 0) for word in sentence] for sentence in target_texts]\n",
        "\n",
        "inputVocabLen = len(input_w2i)\n",
        "targetVocabLen = len(target_w2i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "processing file:  CourseAllocationCS.yml\n",
            "processing file:  ExpectedResults.txt\n",
            "processing file:  TextQuestions.txt\n",
            "processing file:  website.yml\n",
            "processing file:  Artificial_intelligence.yml\n",
            "processing file:  IT.yml\n",
            "processing file:  Sport_games.yml\n",
            "processing file:  computers.yml\n",
            "processing file:  space_and_science.yml\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'mr.', 'hassan', 'mustafa'], ['mr.', 'hassan', 'mustafa', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'a', 'batch', '2018']]\n",
            "[['mr.', 'hassan', 'mustafa', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'a', 'batch', '2018'], ['mr.', 'hassan', 'mustafa', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'b', 'batch', '2018']]\n",
            "[['mr.', 'hassan', 'mustafa', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'b', 'batch', '2018'], ['mr.', 'hassan', 'mustafa', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'c', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'ms.', 'atifa', 'sarwar'], ['ms.', 'atifa', 'sarwar', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'd', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'dr.', 'muhammad', 'arshad', 'islam'], ['dr.', 'muhammad', 'arshad', 'islam', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'e', 'batch', '2018']]\n",
            "[['dr.', 'muhammad', 'arshad', 'islam', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'e', 'batch', '2018'], ['dr.', 'muhammad', 'arshad', 'islam', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'g', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'mr.', 'jawad', 'hassan'], ['mr.', 'jawad', 'hassan', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'f', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'mr.', 'jawad', 'hassan'], ['mr.', 'jawad', 'hassan', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'f', 'batch', '2018']]\n",
            "[['mr.', 'jawad', 'hassan', 'is', 'teaching', 'object', 'oriented', 'programming', 'cs217', 'to', 'section', 'f', 'batch', '2018'], ['mr.', 'jawad', 'hassan', 'is', 'teaching', 'programming', 'fundamentals', 'cs118', 'to', 'section', 'a', 'batch', '2018', 'repeat']]\n",
            "[['mr.', 'jawad', 'hassan', 'is', 'teaching', 'programming', 'fundamentals', 'cs118', 'to', 'section', 'a', 'batch', '2018', 'repeat'], ['mr.', 'jawad', 'hassan', 'is', 'teaching', 'programming', 'fundamentals', 'cs118', 'to', 'section', 'b', 'batch', '2018', 'repeat']]\n",
            "[['mr.', 'jawad', 'hassan', 'is', 'teaching', 'programming', 'fundamentals', 'cs118', 'to', 'section', 'b', 'batch', '2018', 'repeat'], ['mr.', 'jawad', 'hassan', 'is', 'teaching', 'programming', 'fundamentals', 'cs118', 'to', 'section', 'c', 'batch', '2018', 'repeat']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'dr.', 'mehwish', 'hassan'], ['dr.', 'mehwish', 'hassan', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'a', 'batch', '2018']]\n",
            "[['dr.', 'mehwish', 'hassan', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'a', 'batch', '2018'], ['dr.', 'mehwish', 'hassan', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'b', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'ms.', 'mehreen', 'alam'], ['ms.', 'mehreen', 'alam', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'c', 'batch', '2018']]\n",
            "[['ms.', 'mehreen', 'alam', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'c', 'batch', '2018'], ['ms.', 'mehreen', 'alam', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'd', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'dr.', 'adnan', 'saeed'], ['dr.', 'adnan', 'saeed', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'e', 'batch', '2018']]\n",
            "[['dr.', 'adnan', 'saeed', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'e', 'batch', '2018'], ['dr.', 'adnan', 'saeed', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'g', 'batch', '2018']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'dr.', 'ejaz', 'ahmed'], ['dr.', 'ejaz', 'ahmed', 'is', 'teaching', 'database', 'systems', 'cs203', 'to', 'section', 'a', 'batch', '2017']]\n",
            "[['dr.', 'ejaz', 'ahmed', 'is', 'teaching', 'database', 'systems', 'cs203', 'to', 'section', 'a', 'batch', '2017'], ['dr.', 'ejaz', 'ahmed', 'is', 'teaching', 'database', 'systems', 'cs203', 'to', 'section', 'b', 'batch', '2017']]\n",
            "[['what', 'courses', 'are', 'offered', 'by', 'ms.', 'sana', 'hassan'], ['ms.', 'sana', 'hassan', 'is', 'teaching', 'digital', 'logic', 'design', 'ee227', 'to', 'section', 'f', 'batch', '2018']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqHkPZZNexK3",
        "colab_type": "code",
        "outputId": "83025e77-4e30-47d4-835e-1e564906f5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "# Pad all the sequences to same length\n",
        "for i in range(len(x)):\n",
        "\n",
        "    if (len(x[i]) > input_seq_len):\n",
        "        x[i] = x[i][:input_seq_len-1]\n",
        "\n",
        "    # Fill in with padding marker\n",
        "    for k in range(input_seq_len - len(x[i])):\n",
        "        x[i] = x[i] + [input_w2i[marker_pad]]\n",
        "            \n",
        "    if (len(y[i]) > output_seq_len-2):\n",
        "        y[i] = y[i][:output_seq_len-3]\n",
        "\n",
        "    # Add end and begin marker\n",
        "    y[i] = [target_w2i[marker_start]] + y[i] + [target_w2i[marker_end]]\n",
        "\n",
        "    # Fill in with padding marker\n",
        "    for k in range(output_seq_len - len(y[i])):\n",
        "        y[i] = y[i] + [input_w2i[marker_pad]]\n",
        "\n",
        "    if (i < 10):\n",
        "        print(x[i])\n",
        "        print(y[i])\n",
        "\n",
        "        \n",
        "# Train Test Split\n",
        "#X_train,  X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.0)\n",
        "X_train=x;\n",
        "Y_train=y;\n",
        "print(len(X_train))\n",
        "print(len(Y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 25, 7, 11, 20, 13, 37, 133, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 15, 42, 193, 4, 18, 80, 81, 62, 82, 7, 19, 8, 20, 36, 3, 1]\n",
            "[13, 37, 133, 2, 28, 93, 94, 56, 95, 8, 29, 10, 30, 51, 1]\n",
            "[2, 15, 42, 193, 4, 18, 80, 81, 62, 82, 7, 19, 102, 20, 36, 3, 1]\n",
            "[13, 37, 133, 2, 28, 93, 94, 56, 95, 8, 29, 134, 30, 51, 1]\n",
            "[2, 15, 42, 193, 4, 18, 80, 81, 62, 82, 7, 19, 103, 20, 36, 3, 1]\n",
            "[3, 25, 7, 11, 20, 34, 351, 352, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 38, 496, 497, 4, 18, 80, 81, 62, 82, 7, 19, 144, 20, 36, 3, 1]\n",
            "[3, 25, 7, 11, 20, 14, 85, 96, 174, 1, 1, 1, 1, 1, 1]\n",
            "[2, 16, 123, 145, 275, 4, 18, 80, 81, 62, 82, 7, 19, 124, 20, 36, 3]\n",
            "[14, 85, 96, 174, 2, 28, 93, 94, 56, 95, 8, 29, 97, 30, 51]\n",
            "[2, 16, 123, 145, 275, 4, 18, 80, 81, 62, 82, 7, 19, 276, 20, 36, 3]\n",
            "[3, 25, 7, 11, 20, 13, 86, 37, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 15, 125, 42, 4, 18, 80, 81, 62, 82, 7, 19, 104, 20, 36, 3, 1]\n",
            "[3, 25, 7, 11, 20, 13, 86, 37, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 15, 125, 42, 4, 18, 80, 81, 62, 82, 7, 19, 104, 20, 36, 3, 1]\n",
            "[13, 86, 37, 2, 28, 93, 94, 56, 95, 8, 29, 353, 30, 51, 1]\n",
            "[2, 15, 125, 42, 4, 18, 62, 194, 195, 7, 19, 8, 20, 36, 196, 3, 1]\n",
            "[13, 86, 37, 2, 28, 56, 175, 176, 8, 29, 10, 30, 51, 177, 1]\n",
            "[2, 15, 125, 42, 4, 18, 62, 194, 195, 7, 19, 102, 20, 36, 196, 3, 1]\n",
            "350\n",
            "350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHpxAtyzexK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stub code sourced from Neural machine translator for English2German translation\n",
        "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
        "\n",
        "# feed data into placeholders\n",
        "def feed_dict(x, y, batch_size = 64):\n",
        "    feed = {}\n",
        "    \n",
        "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
        "    \n",
        "    for i in range(input_seq_len):\n",
        "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
        "        \n",
        "    for i in range(output_seq_len):\n",
        "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
        "        \n",
        "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = target_w2i[marker_pad], dtype = np.int32)\n",
        "    \n",
        "    for i in range(output_seq_len-1):\n",
        "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
        "        target = feed[decoder_inputs[i+1].name]\n",
        "        for j in range(batch_size):\n",
        "            if target[j] == target_w2i[marker_pad]:\n",
        "                batch_weights[j] = 0.0\n",
        "        feed[target_weights[i].name] = batch_weights\n",
        "        \n",
        "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
        "    \n",
        "    return feed\n",
        "\n",
        "# define our loss function\n",
        "\n",
        "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
        "def sampled_loss(labels, logits):\n",
        "    return tf.nn.sampled_softmax_loss(\n",
        "                        weights = w_t,\n",
        "                        biases = b,\n",
        "                        labels = tf.reshape(labels, [-1, 1]),\n",
        "                        inputs = logits,\n",
        "                        num_sampled = 512,\n",
        "                        num_classes = targetVocabLen)\n",
        "\n",
        "# decode output sequence\n",
        "def decode_output(output_seq):\n",
        "    words = []\n",
        "    for i in range(output_seq_len):\n",
        "        smax = softmax(output_seq[i])\n",
        "        idx = np.argmax(smax)\n",
        "        words.append(target_i2w[idx])\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LHwDRw5exLA",
        "colab_type": "code",
        "outputId": "fe90dfc7-70e2-4aaa-c70c-2041906a63aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Stub code sourced from Neural machine translator for English2German translation\n",
        "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
        "\n",
        "# Defining placeholders\n",
        "# The first None means the batch size, and the batch size is unknown since user can set it. \n",
        "# The second None means the lengths of sentences. \n",
        "\n",
        "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) \n",
        "                  for i in range(input_seq_len)]\n",
        "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) \n",
        "                  for i in range(output_seq_len)]\n",
        "\n",
        "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
        "# add one more target\n",
        "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
        "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], \n",
        "                                 name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
        "\n",
        "# output projection\n",
        "size = 512\n",
        "w_t = tf.get_variable('proj_w', [targetVocabLen, size], tf.float32)\n",
        "b = tf.get_variable('proj_b', [targetVocabLen], tf.float32)\n",
        "w = tf.transpose(w_t)\n",
        "output_projection = (w, b)\n",
        "\n",
        "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "                                            encoder_inputs,\n",
        "                                            decoder_inputs,\n",
        "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
        "                                            num_encoder_symbols = inputVocabLen,\n",
        "                                            num_decoder_symbols = targetVocabLen,\n",
        "                                            embedding_size = 100,\n",
        "                                            feed_previous = False,\n",
        "                                            output_projection = output_projection,\n",
        "                                            dtype = tf.float32)\n",
        "\n",
        "# Weighted cross-entropy loss for a sequence of logits\n",
        "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-10-caba91875f59>:23: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:859: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/rnn/python/ops/core_rnn_cell.py:104: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "b7VrQx1DexLF",
        "colab_type": "code",
        "outputId": "34e69ce4-3d65-4cca-c59c-1ff5d5d2c00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Stub code sourced from Neural machine translator for English2German translation\n",
        "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
        "\n",
        "# ops and hyperparameters\n",
        "learning_rate = 7e-3\n",
        "batch_size = 96\n",
        "steps = 10000\n",
        "#num_epochs = 2\n",
        "\n",
        "# ops for projecting outputs\n",
        "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
        "\n",
        "# training op\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "# tf.train.RMSPropOptimizer\n",
        "\n",
        "# init op\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Loss values appended to plot diagram\n",
        "losses = []\n",
        "\n",
        "# Save checkpoint to restore the model later \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(steps):\n",
        "        feed = feed_dict(X_train, Y_train, batch_size)\n",
        "        sess.run(optimizer, feed_dict = feed)\n",
        "        \n",
        "        if step % 500 == 0:\n",
        "            loss_value = sess.run(loss, feed_dict = feed)\n",
        "            print('step: {}, loss: {}'.format(step, loss_value))\n",
        "            losses.append(loss_value)\n",
        "            \n",
        "    saver.save(sess, 'checkpoints/', global_step=step)\n",
        "    print('Checkpoint is saved')\n",
        "    \n",
        "# plot the losses\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.plot(losses, linewidth = 1)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Losses')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 9500, loss: 4.131120681762695\n",
            "step: 9500, loss: 4.131120681762695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAASFZdCexLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To predict response (inference) use the same model as defined above with forward feed\n",
        "\n",
        "def generateReply(humanMsg):\n",
        "    import tensorflow as tf\n",
        "    \n",
        "    if (len(humanMsg) == 0):\n",
        "        return ''\n",
        "    \n",
        "    with tf.Graph().as_default():\n",
        "        \n",
        "        replyMsg = \"\"\n",
        "\n",
        "        # same format as in model building\n",
        "        encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], \n",
        "                                         name = 'encoder{}'.format(i)) for i in range(15)]\n",
        "        decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], \n",
        "                                         name = 'decoder{}'.format(i)) for i in range(17)]\n",
        "\n",
        "        # output projection\n",
        "        size = 512\n",
        "        w_t = tf.get_variable('proj_w', [targetVocabLen, size], tf.float32)\n",
        "        b = tf.get_variable('proj_b', [targetVocabLen], tf.float32)\n",
        "        w = tf.transpose(w_t)\n",
        "        output_projection = (w, b)\n",
        "\n",
        "        # feed_previous is set to true so that output at time t can be fed as input at time t+1\n",
        "        outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "                                                    encoder_inputs,\n",
        "                                                    decoder_inputs,\n",
        "                                                    tf.contrib.rnn.BasicLSTMCell(size),\n",
        "                                                    num_encoder_symbols = inputVocabLen,\n",
        "                                                    num_decoder_symbols = targetVocabLen,\n",
        "                                                    embedding_size = 100,\n",
        "                                                    feed_previous = True,\n",
        "                                                    output_projection = output_projection,\n",
        "                                                    dtype = tf.float32)\n",
        "        # ops for projecting outputs\n",
        "        outputs_proj = [tf.matmul(outputs[i], \n",
        "                        output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
        "\n",
        "        ## Clean and Format incoming msg by humans.\n",
        "        ## It is better to do the same clean/format as the data preprocessing steps\n",
        "        ## for the algorithm to predict next words more accurately\n",
        "        msgLowerCase = [w.lower() for w in nltk.word_tokenize(humanMsg)]\n",
        "        msg = [w for w in msgLowerCase if permissible_chars(w)]\n",
        "        if len(msg) > input_seq_len:\n",
        "            msg = msg[0:input_seq_len-1]\n",
        "\n",
        "        human_msg_encoded = [input_w2i.get(word, 0) for word in msg]\n",
        "\n",
        "        # Fill in with padding marker\n",
        "        for k in range(input_seq_len - len(human_msg_encoded)):\n",
        "            human_msg_encoded = human_msg_encoded + [input_w2i[marker_pad]]\n",
        "\n",
        "        # restore all variables - use the last checkpoint saved\n",
        "        saver = tf.train.Saver()\n",
        "        path = tf.train.latest_checkpoint('checkpoints')\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            # restore\n",
        "            saver.restore(sess, path)\n",
        "\n",
        "            # feed data into placeholders\n",
        "            feed = {}\n",
        "            for i in range(input_seq_len):\n",
        "                feed[encoder_inputs[i].name] = np.array([human_msg_encoded[i]], dtype = np.int32)\n",
        "\n",
        "            feed[decoder_inputs[0].name] = np.array([target_w2i[marker_start]], dtype = np.int32)\n",
        "\n",
        "            # translate\n",
        "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
        "\n",
        "            ouput_seq = [output_sequences[j][0] for j in range(output_seq_len)]\n",
        "            #decode output sequence\n",
        "            words = decode_output(ouput_seq)\n",
        "\n",
        "            for i in range(len(words)):\n",
        "                if words[i] not in [marker_end, marker_pad, marker_start]:\n",
        "                    replyMsg += words[i] + ' '\n",
        "                             \n",
        "        #print(replyMsg)\n",
        "        return replyMsg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWItausJUSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bot_reply = generateReply('When is AI class on Monday?')\n",
        "print(bot_reply)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AHrMbFI7bKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow8B8ylRUWNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "testquestions_docs=[]\n",
        "with open ('/content/drive/My Drive/TestQuestions.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        testquestions_docs.append(line)\n",
        "\n",
        "print(\"Number of test questions:\",len(testquestions_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7PElunNYhlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expectedanswers_docs=[]\n",
        "with open ('/content/drive/My Drive/TestAnswers.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        expectedanswers_docs.append(line)\n",
        "\n",
        "print(\"Number of test questions:\",len(expectedanswers_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnEovTywbANi",
        "colab_type": "text"
      },
      "source": [
        "**CALCULATING BLEU SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICvPUAQd3GBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum=0\n",
        "for i in range(0,9,1):\n",
        "  Question = testquestions_docs[i]\n",
        "  print(\"Question:\")\n",
        "  print(Question)\n",
        "  expected_data = expectedanswers_docs[i].lower()\n",
        "  print(\"Expected response:\")\n",
        "  print( expected_data)\n",
        "  actual_data = generateReply(Question)\n",
        "  print(\"Actual response:\")\n",
        "  print( actual_data)\n",
        "  actual_response = word_tokenize(actual_data)\n",
        "  expected_response = word_tokenize(expected_data)\n",
        "  BLEUscore = nltk.translate.bleu_score.sentence_bleu([expected_response], actual_response, weights = (0.5, 0.5))\n",
        "  print(\"BLEU SCORE FOR RESPONSE \" +str(i)+\": \"+str(BLEUscore))\n",
        "  sum=sum+BLEUscore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj947iMHJE_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#option = \"\"\n",
        "\n",
        "#while(option !=\"yes\"):\n",
        "#name1 = input(\"Enter Question : \") \n",
        "  #bot_reply = generateReply(name1)\n",
        "  #print(\"To end conversation enter yes else enter no\")\n",
        "  #option = input(\"Enter option : \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me8rynQR2vqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum=sum/10\n",
        "print(\"BLEU SCORE: \"+str(sum))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhaHw6LLTpXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "actual_response = word_tokenize(\"I am Maham\")\n",
        "expected_response = word_tokenize(\"I am is Maham\")\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([expected_response], actual_response, weights = (0.5, 0.5))\n",
        "print(BLEUscore)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}